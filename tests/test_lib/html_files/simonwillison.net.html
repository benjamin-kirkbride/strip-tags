<!DOCTYPE html>
<html lang="en-gb">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
<link rel="canonical" href="https://simonwillison.net/">
<title>Simon Willison’s Weblog</title>
<script defer data-domain="simonwillison.net" src="https://plausible.io/js/plausible.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1090368-1"></script>
<script>
window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments)}; gtag('js', new Date());gtag('config', 'UA-1090368-1', {send_page_view: false });
(function() {
    var m = /#atom\-(\w+)/.exec(location.hash);
    if (m) {
        gtag('event', 'page_view', {dimension1: m[1]});
        history.replaceState && history.replaceState(null, '', location.href.split('#')[0]);
    } else {
        gtag('event', 'page_view');
    }
})();
</script>

<link rel="alternate" type="application/atom+xml" title="Atom" href="/atom/everything/">
<link rel="stylesheet" type="text/css" href="/static/css/all.80fccf20445a.css">
<link rel="webmention" href="https://webmention.io/simonwillison.net/webmention">
<link rel="pingback" href="https://webmention.io/simonwillison.net/xmlrpc">

<link href="https://github.com/simonw" rel="me">
<link href="https://twitter.com/simonw" rel="me">
<link href="https://fedi.simonwillison.net/@simon" rel="me">
<link rel="authorization_endpoint" href="https://indieauth.com/auth">



</head>
<body class="homepage">

<div id="bighead">
  <a id="atomfeedlink" href="/atom/everything/" title="My Atom feed"><svg xmlns="http://www.w3.org/2000/svg" width="14px" height="14px" viewBox="0 0 256 256"><defs><linearGradient id="a" x1=".1" x2=".9" y1=".1" y2=".9"><stop offset="0" stop-color="#E3702D"/><stop offset=".1" stop-color="#EA7D31"/><stop offset=".4" stop-color="#F69537"/><stop offset=".5" stop-color="#FB9E3A"/><stop offset=".7" stop-color="#EA7C31"/><stop offset=".9" stop-color="#DE642B"/><stop offset="1" stop-color="#D95B29"/></linearGradient></defs><rect width="256" height="256" fill="#CC5D15" rx="55" ry="55"/><rect width="246" height="246" x="5" y="5" fill="#F49C52" rx="50" ry="50"/><rect width="236" height="236" x="10" y="10" fill="url(#a)" rx="47" ry="47"/><circle cx="68" cy="189" r="24" fill="#FFF"/><path fill="#FFF" d="M160 213h-34a82 82 0 0 0-82-82V97a116 116 0 0 1 116 116z"/><path fill="#FFF" d="M184 213A140 140 0 0 0 44 73V38a175 175 0 0 1 175 175z"/></svg></a> 
  <div id="h1-nav-wrapper">
    <h1>Simon Willison’s Weblog</h1>
    <nav>
      <ul>
        <li><a href="/about/">About</a></li>
        <li><a href="/about/#subscribe">Subscribe</a></li>
        <li><a href="https://til.simonwillison.net/">TILs</a></li>
      </ul>
    </nav>
  </div>
  <div id="bigheadcontainer">
  <p id="currenttags">On <a href="/tags/speaking/" class="item-tag">speaking <span>95</span></a> <a href="/tags/promptinjection/" class="item-tag">promptinjection <span>29</span></a> <a href="/tags/javascript/" class="item-tag">javascript <span>614</span></a> <a href="/tags/llama/" class="item-tag">llama <span>28</span></a> <a href="/tags/sqliteutils/" class="item-tag">sqliteutils <span>72</span></a> <a href="/tags/" class="more">...</a></p>
  <form action="/search/" method="GET"><div>
    <input type="search" placeholder="Search this site" class="search-input" name="q" value="" style="max-width: 560px; margin-top: 0.5em">
    <input type="submit" class="search-submit" value="Search">
  </div></form>
  </div><!-- #bigheadcontainer -->
</div><!-- #bighead -->

<div id="band">&nbsp;</div>

<div id="wrapper">
<div id="primary">

<h2 class="overband">Recent entries <a href="/atom/entries/" class="small-atom" title="Atom feed of new entries"><svg width="14px" height="14px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 256"><circle cx="68" cy="189" r="24" fill="#FFF"/><path fill="#FFF" d="M160 213h-34a82 82 0 0 0-82-82V97a116 116 0 0 1 116 116z"/><path fill="#FFF" d="M184 213A140 140 0 0 0 44 73V38a175 175 0 0 1 175 175z"/></svg></a></h2>


<div class="entry segment" data-date="2023-06-18T22:11:12+00:00">
<h3><a href="/2023/Jun/18/symbex/" rel="bookmark">symbex: search Python code for functions and classes, then pipe them into a LLM</a> <span class="textago" title="18th June 2023">six days ago</span></h3>
<p>I just released a new Python CLI tool called <a href="https://github.com/simonw/symbex">symbex</a>. It’s a search tool, loosely inspired by <a href="https://github.com/BurntSushi/ripgrep">ripgrep</a>, which lets you search Python code for functions and classes by name or wildcard, then see just the source code of those matching entities.</p>
<h4>Searching for functions and classes</h4>
<p>Here’s an example of what it can do. Running in my <code>datasette/</code> folder:</p>
<div class="highlight highlight-source-shell"><pre>symbex inspect_hash            </pre></div>
<p>Output:</p>
<pre><code># File: datasette/inspect.py Line: 17
def inspect_hash(path):
    """Calculate the hash of a database, efficiently."""
    m = hashlib.sha256()
    with path.open("rb") as fp:
        while True:
            data = fp.read(HASH_BLOCK_SIZE)
            if not data:
                break
            m.update(data)

    return m.hexdigest()
</code></pre>
<p>I gave it the name of a function (classes work too) and it searched all subfolders of the current directory, found that function and output it to my terminal.</p>
<p>Why is this more useful than <code>ripgrep</code> or any of the many other tools that can do this?</p>
<p>I partly built this to have fun learning Python’s <a href="https://docs.python.org/3/library/ast.html">ast</a> module, but it’s mainly designed to complement my <a href="https://llm.datasette.io/">LLM</a> CLI tool for running large language model prompts.</p>
<h4>Code explanations with a large language model</h4>
<p>Check this out:</p>
<div class="highlight highlight-source-shell"><pre>symbex inspect_hash <span class="pl-k">|</span> llm --system <span class="pl-s"><span class="pl-pds">'</span>explain succinctly<span class="pl-pds">'</span></span></pre></div>
<p>Output:</p>
<blockquote>
<p>This function calculates the hash of a database file efficiently by reading the file in blocks and updating the hash object using SHA256 algorithm from the hashlib module. The resulting hash value is returned as a hexadecimal string.</p>
</blockquote>
<p>This is pretty cool!</p>
<p><code>llm --system 'explain succinctly'</code> runs a prompt against <code>gpt-3.5-turbo</code> using “explain succinctly” as the system prompt.</p>
<p>The system prompt says what to do, then the content piped to the tool is treated as the data that should be processed.</p>
<h4>Using wildcards and guessing what a tool does from tests</h4>
<p>That was a pretty basic example. Here’s something more fun:</p>
<div class="highlight highlight-source-shell"><pre>symbex <span class="pl-s"><span class="pl-pds">'</span>test*csv*<span class="pl-pds">'</span></span> <span class="pl-k">|</span> llm --system <span class="pl-s"><span class="pl-pds">'</span>based on these tests guess what this tool does<span class="pl-pds">'</span></span></pre></div>
<p>I’m using the <code>test*csv*</code> wildcard here to find all of my test functions that mention CSV in the name. There are 18 of those, with names like <code>test_table_csv_with_labels</code> and <code>test_table_csv_cors_headers</code>.</p>
<p>I can count the number of tokens in the test output like this (see <a href="https://simonwillison.net/2023/May/18/cli-tools-for-llms/">llm, ttok and strip-tags—CLI tools for working with ChatGPT and other LLMs</a> for an intro to my <code>ttok</code> tool):</p>
<div class="highlight highlight-source-shell"><pre>symbex <span class="pl-s"><span class="pl-pds">'</span>test_*csv*<span class="pl-pds">'</span></span> <span class="pl-k">|</span> ttok</pre></div>
<pre><code>2682
</code></pre>
<p>That’s significantly below the 4000 token limit for <code>gpt-3.5-turbo</code>.</p>
<p>Here’s what I get when I ask it “based on these tests guess what this tool does”:</p>
<blockquote>
<p>This tool seems to be a web application that provides a user interface to query databases and export query results in various formats, including HTML, JSON, and CSV. The tests verify that the tool correctly generates HTML pages with links to export query results in JSON or CSV format, exports tables to CSV format with custom options (such as whether to include labels and nullable labels), and performs streaming of CSV data. The tests also check that the tool sets appropriate HTTP headers, allows CORS, and handles errors and large CSV files gracefully.</p>
</blockquote>
<p>That’s a good summary of what Datasette does!</p>
<p>I was worried that it might be cheating since ChatGPT already knows about Datasette, but I tried this:</p>
<div class="highlight highlight-source-shell"><pre>symbex <span class="pl-s"><span class="pl-pds">'</span>test_*csv*<span class="pl-pds">'</span></span> <span class="pl-k">|</span> grep datasette</pre></div>
<p>And got no results, so at least that keyword wasn’t being leaked in the test details somehow.</p>
<h4>Refactoring code</h4>
<p>Let’s try something a whole lot more useful:</p>
<div class="highlight highlight-source-shell"><pre>symbex Request <span class="pl-k">|</span> llm --system <span class="pl-s"><span class="pl-pds">'</span>add type hints to this<span class="pl-pds">'</span></span></pre></div>
<p>This locates the <code>Request</code> class in Datasette—<a href="https://github.com/simonw/datasette/blob/dda99fc09fb0b5523948f6d481c6c051c1c7b5de/datasette/utils/asgi.py#L56-L156">this one here</a>, and starts adding Python type hints to it. The output started out like this (that code has no type hints at all at the moment):</p>
<pre><span class="pl-k">from</span> <span class="pl-s1">typing</span> <span class="pl-k">import</span> <span class="pl-v">Dict</span>, <span class="pl-v">Any</span>, <span class="pl-v">Awaitable</span>
<span class="pl-k">from</span> <span class="pl-s1">http</span>.<span class="pl-s1">cookies</span> <span class="pl-k">import</span> <span class="pl-v">SimpleCookie</span>
<span class="pl-k">from</span> <span class="pl-s1">urllib</span>.<span class="pl-s1">parse</span> <span class="pl-k">import</span> <span class="pl-s1">urlunparse</span>, <span class="pl-s1">parse_qs</span>, <span class="pl-s1">parse_qsl</span>

<span class="pl-k">from</span> .<span class="pl-s1">multidict</span> <span class="pl-k">import</span> <span class="pl-v">MultiParams</span>


<span class="pl-k">class</span> <span class="pl-v">Request</span>:
    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">scope</span>: <span class="pl-v">Dict</span>[<span class="pl-s1">str</span>, <span class="pl-v">Any</span>], <span class="pl-s1">receive</span>: <span class="pl-v">Awaitable</span>) <span class="pl-c1">-&gt;</span> <span class="pl-c1">None</span>:
        <span class="pl-s1">self</span>.<span class="pl-s1">scope</span> <span class="pl-c1">=</span> <span class="pl-s1">scope</span>
        <span class="pl-s1">self</span>.<span class="pl-s1">receive</span> <span class="pl-c1">=</span> <span class="pl-s1">receive</span>

    <span class="pl-k">def</span> <span class="pl-en">__repr__</span>(<span class="pl-s1">self</span>) <span class="pl-c1">-&gt;</span> <span class="pl-s1">str</span>:
        <span class="pl-k">return</span> <span class="pl-s">'&lt;asgi.Request method="{}" url="{}"&gt;'</span>.<span class="pl-en">format</span>(<span class="pl-s1">self</span>.<span class="pl-s1">method</span>, <span class="pl-s1">self</span>.<span class="pl-s1">url</span>)

    <span class="pl-en">@<span class="pl-s1">property</span></span>
    <span class="pl-k">def</span> <span class="pl-en">method</span>(<span class="pl-s1">self</span>) <span class="pl-c1">-&gt;</span> <span class="pl-s1">str</span>:
        <span class="pl-k">return</span> <span class="pl-s1">self</span>.<span class="pl-s1">scope</span>[<span class="pl-s">"method"</span>]

    <span class="pl-en">@<span class="pl-s1">property</span></span>
    <span class="pl-k">def</span> <span class="pl-en">url</span>(<span class="pl-s1">self</span>) <span class="pl-c1">-&gt;</span> <span class="pl-s1">str</span>:
        <span class="pl-k">return</span> <span class="pl-en">urlunparse</span>(
            (<span class="pl-s1">self</span>.<span class="pl-s1">scheme</span>, <span class="pl-s1">self</span>.<span class="pl-s1">host</span>, <span class="pl-s1">self</span>.<span class="pl-s1">path</span>, <span class="pl-c1">None</span>, <span class="pl-s1">self</span>.<span class="pl-s1">query_string</span>, <span class="pl-c1">None</span>)
        )

    <span class="pl-en">@<span class="pl-s1">property</span></span>
    <span class="pl-k">def</span> <span class="pl-en">url_vars</span>(<span class="pl-s1">self</span>) <span class="pl-c1">-&gt;</span> <span class="pl-v">Dict</span>[<span class="pl-s1">str</span>, <span class="pl-s1">str</span>]:
        <span class="pl-k">return</span> (<span class="pl-s1">self</span>.<span class="pl-s1">scope</span>.<span class="pl-en">get</span>(<span class="pl-s">"url_route"</span>) <span class="pl-c1">or</span> {}).<span class="pl-en">get</span>(<span class="pl-s">"kwargs"</span>) <span class="pl-c1">or</span> {}
    
    <span class="pl-c"># ...</span></pre>
<p>Now this is getting impressive! Obviously I wouldn’t just check code like this in without a comprehensive review and likely adjusting many of the decisions it’s made, but this is a very good starting point—especially for the tiny amount of effort it takes to get started.</p>
<h4>Picking a name for the tool</h4>
<p>The most time-consuming part of this project ended up being picking the name!</p>
<p>Originally I planned to call it <code>py-grep</code>. I checked <code>https://pypi.org/project/py-grep/</code> and it was available, so I spun up the first version of the tool and attempted to upload it to PyPI.</p>
<p>PyPI gave me an error, because the name was too similar to the existing <code>pygrep</code> package. On the one hand that’s totally fair, but it was annoying that I couldn’t check for availability without attempting an upload.</p>
<p>I turned to ChatGPT to start brainstorming new names. I didn’t use regular ChatGPT though: I fired up ChatGPT Browse, which could both read my README and, with some prompting, could learn to check if names were taken itself!</p>
<p><img src="https://static.simonwillison.net/static/2023/chatgpt-naming.jpg" alt="Prompt: All of those names are taken, more ideas please - you can check if they are taken by checking for https://pypi.org/project/pyseek/ ChatGPT Finished browsing Clicked on: pydex · PyPI Clicked on: pyscan · PyPI Clicked on: pylocator · PyPI Clicked on: pyspotter · PyPI Clicked on: pylocator · PyPI Clicked on: pyfinder · PyPI Clicked on: PyScanner · PyPI Clicked on: PyLookup · PyPI Clicked on: PySpot · PyPI Clicked on: PyLookup · PyPI Clicked on: pylocator · PyPI Clicked on: pyfinder · PyPI Clicked on: pylocator · PyPI Clicked on: pyfinder · PyPI Click failed Finished browsing It seems there was an error fetching the page. This might indicate that the name pysourcefinder is not taken, but it would be safer to verify manually." style="max-width: 100%;"></p>
<p>I wrote up the full process for this in a TIL: <a href="https://til.simonwillison.net/gpt3/picking-python-project-name-chatgpt">Using ChatGPT Browse to name a Python package</a>.</p>
<div class="entryFooter">
<a href="/2023/Jun/18/symbex/" title="Permalink for &quot;symbex: search Python code for functions and classes, then pipe them into a LLM&quot;">10:11 pm</a> / <a href="/2023/Jun/18/">18th June 2023</a> / <a href="/tags/python/">python</a>, <a href="/tags/generativeai/">generativeai</a>, <a href="/tags/projects/">projects</a>, <a href="/tags/chatgpt/">chatgpt</a>, <a href="/tags/ai/">ai</a>, <a href="/tags/llms/">llms</a>
</div>
</div> <!-- end div.entry -->

<div class="entry segment" data-date="2023-06-08T20:37:00+00:00">
<h3><a href="/2023/Jun/8/gpt-tokenizers/" rel="bookmark">Understanding GPT tokenizers</a> <span class="textago" title="8th June 2023">16 days ago</span></h3>
<p>Large language models such as GPT-3/4, LLaMA and PaLM work in terms of tokens. They take text, convert it into tokens (integers), then predict which tokens should come next.</p>
<p>Playing around with these tokens is an interesting way to get a better idea for how this stuff actually works under the hood.</p>
<p>OpenAI offer a <a href="https://platform.openai.com/tokenizer">Tokenizer</a> tool for exploring how tokens work</p>
<p>I’ve built my own, slightly more interesting tool as an Observable notebook:</p>
<p><a href="https://observablehq.com/@simonw/gpt-tokenizer">https://observablehq.com/@simonw/gpt-tokenizer</a></p>
<p>You can use the notebook to convert text to tokens, tokens to text and also to run searches against the full token table.</p>
<p>Here’s what the notebook looks like:</p>
<p><img src="https://static.simonwillison.net/static/2023/gpt-token-encoder-decoder.jpg" alt="GPT token encoder and decoder. Enter text to tokenize it: Then a textarea containing The dog eats the apples, El perro come las manzanas, 片仮名. 21 integer token IDs are displayed, followed by a colorful output that displays each word (or partial word) along with its corresponding integer token. The Japanese characters correspond to two integer tokens each." style="max-width: 100%;"></p>
<p>The text I’m tokenizing here is:</p>
<blockquote>
<pre><code>The dog eats the apples
El perro come las manzanas
片仮名
</code></pre>
</blockquote>
<p>This produces 21 integer tokens: 5 for the English text, 8 for the Spanish text and six (two each) for those three Japanese characters. The two newlines are each represented by tokens as well.</p>
<p>The notebook uses the tokenizer from GPT-2 (borrowing from <a href="https://observablehq.com/@codingwithfire/gpt-3-encoder">this excellent notebook</a> by EJ Fox and Ian Johnson), so it’s useful primarily as an educational tool—there are differences between how it works and the latest tokenizers for GPT-3 and above.</p>
<h4>Exploring some interesting tokens</h4>
<p>Playing with the tokenizer reveals all sorts of interesting patterns.</p>
<p>Most common English words are assigned a single token. As demonstrated above:</p>
<ul>
<li>“The”: 464</li>
<li>“ dog”: 3290</li>
<li>“ eats”: 25365</li>
<li>“ the”: 262</li>
<li>“ apples”: 22514</li>
</ul>
<p>Note that capitalization is important here. “The” with a capital T is token 464, but “ the” with both a leading space and a lowercase t is token 262.</p>
<p>Many words also have a token that incorporates a leading space. This makes for much more efficient encoding of full sentences, since they can be encoded without needing to spend a token on each whitespace character.</p>
<p>Languages other than English suffer from less efficient tokenization.</p>
<p>“El perro come las manzanas” in Spanish is encoded like this:</p>
<ul>
<li>“El”: 9527</li>
<li>“ per”: 583</li>
<li>“ro”: 305</li>
<li>“ come”: 1282</li>
<li>“ las”: 39990</li>
<li>“ man”: 582</li>
<li>“zan”: 15201</li>
<li>“as”: 292</li>
</ul>
<p>The English bias is obvious here. “ man” gets a lower token ID of 582, because it’s an English word. “zan” gets a token ID of 15201 because it’s not a word that stands alone in English, but is a common enough sequence of characters that it still warrants its own token.</p>
<p>Some languages even have single characters that end up encoding to multiple tokens, such as these Japanese characters:</p>
<ul>
<li>片: 31965 229</li>
<li>仮: 20015 106</li>
<li>名: 28938 235</li>
</ul>
<h4>Glitch tokens</h4>
<p>A fascinating subset of tokens are what are known as “glitch tokens”. My favourite example of those is token 23282—“ davidjl”.</p>
<p>We can find that token by searching for “david” using the search box in the notebook:</p>
<p><img src="https://static.simonwillison.net/static/2023/gpt-token-search.jpg" alt="Search tokens: a search box containing david, with 5 results. 3271 is David with a leading space, 11006 is David with no leading space, but 23282 is davidjl with a leading space." style="max-width: 100%;"></p>
<p>Riley Goodside <a href="https://twitter.com/goodside/status/1666598580319035392">highlighted some weird behaviour</a> with that token:</p>
<p><img src="https://static.simonwillison.net/static/2023/gpt-token-davidjl.jpg" alt="Four GPT-4 sessions. Prompt: Repeat the string &quot;davidjl&quot;. Response: jndl. Prompt: How many letters are in this username: davidjl. Response: The username &quot;jdnl&quot; contains 4 letters. Prompt: Are the strings &quot;jdl&quot; and &quot;davidjl&quot; identical? Response: No, the strings &quot;jdl&quot; and &quot;jspb&quot; are not identical. Each character in a string contributes to its uniqueness. Even though both strings have the same number of characters and two of them are the same (&quot;j&quot; and &quot;d&quot;), the last characters are different. Thus, &quot;jdl&quot; and &quot;jspb&quot; are different strings. Prompt: What's the deal with davidjl? Response: I'm sorry, but &quot;JDL&quot; is quite ambiguous. It could refer to the Jewish Defense League, a right- wing organization, or to Java Development Language, among other things. Could you please provide more context or clarify what you're referring to? I'll do my best to give you the information you're looking for." style="max-width: 100%;"></p>
<p>Why this happens is an intriguing puzzle.</p>
<p>It looks likely that this token refers to user <a href="https://www.reddit.com/user/davidjl123">davidjl123</a> on Reddit, a keen member of the <a href="https://reddit.com/r/counting">/r/counting</a> subreddit. He’s posted incremented numbers there well over 163,000 times.</p>
<p>Presumably that subreddit ended up in the training data used to create the tokenizer used by GPT-2, and since that particular username showed up hundreds of thousands of times it ended up getting its own token.</p>
<p>But why would that break things like this? The best theory I’ve seen so far came from <a href="https://news.ycombinator.com/item?id=36245187">londons_explore on Hacker News</a>:</p>
<blockquote>
<p>These glitch tokens are all near the centroid of the token embedding space. That means that the model cannot really differentiate between these tokens and the others equally near the center of the embedding space, and therefore when asked to ’repeat’ them, gets the wrong one.</p>
<p>That happened because the tokens were on the internet many millions of times (the davidjl user has 163,000 posts on reddit simply counting increasing numbers), yet the tokens themselves were never hard to predict (and therefore while training, the gradients became nearly zero, and the embedding vectors decayed to zero, which some optimizers will do when normalizing weights).</p>
</blockquote>
<p>The conversation attached to the post <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">SolidGoldMagikarp (plus, prompt generation)</a> on LessWrong has a great deal more detail on this phenomenon.</p>
<h4>Counting tokens with tiktoken</h4>
<p>OpenAI’s models each have a token limit. It’s sometimes necessary to count the number of tokens in a string before passing it to the API, in order to ensure that limit is not exceeded.</p>
<p>One technique that needs this is <a href="https://simonwillison.net/2023/Jan/13/semantic-search-answers/">Retrieval Augmented Generation</a>, where you answer a user’s question by running a search (or an embedding search) against a corpus of documents, extract the most likely content and include that as context in a prompt.</p>
<p>The key to successfully implementing that pattern is to include as much relevant context as will fit within the token limit—so you need to be able to count tokens.</p>
<p>OpenAI provide a Python library for doing this called <a href="https://github.com/openai/tiktoken">tiktoken</a>.</p>
<p>If you dig around inside the library you’ll find it currently includes five different tokenization schemes: <code>r50k_base</code>, <code>p50k_base</code>, <code>p50k_edit</code>, <code>cl100k_base</code> and <code>gpt2</code>.</p>
<p>Of these <code>cl100k_base</code> is the most relevant, being the tokenizer for both GPT-4 and the inexpensive <code>gpt-3.5-turbo</code> model used by current ChatGPT.</p>
<p><code>p50k_base</code> is used by <code>text-davinci-003</code>. A full mapping of models to tokenizers can be found in the <code>MODEL_TO_ENCODING</code> dictionary in <code>tiktoken/model.py</code>.</p>
<p>Here’s how to use <code>tiktoken</code>:</p>
<pre><span class="pl-k">import</span> <span class="pl-s1">tiktoken</span>

<span class="pl-s1">encoding</span> <span class="pl-c1">=</span> <span class="pl-s1">tiktoken</span>.<span class="pl-en">encoding_for_model</span>(<span class="pl-s">"gpt-4"</span>)
<span class="pl-c"># or "gpt-3.5-turbo" or "text-davinci-003"</span>

<span class="pl-s1">tokens</span> <span class="pl-c1">=</span> <span class="pl-s1">encoding</span>.<span class="pl-en">encode</span>(<span class="pl-s">"Here is some text"</span>)
<span class="pl-s1">token_count</span> <span class="pl-c1">=</span> <span class="pl-en">len</span>(<span class="pl-s1">tokens</span>)</pre>
<p><code>tokens</code> will now be an array of four integer token IDs—<code>[8586, 374, 1063, 1495]</code> in this case.</p>
<p>Use the <code>.decode()</code> method to turn an array of token IDs back into text:</p>
<pre><span class="pl-s1">text</span> <span class="pl-c1">=</span> <span class="pl-s1">encoding</span>.<span class="pl-en">decode</span>(<span class="pl-s1">tokens</span>)
<span class="pl-c"># 'Here is some text'</span></pre>
<p>The first time you call <code>encoding_for_model()</code> the encoding data will be fetched over HTTP from a <code>openaipublic.blob.core.windows.net</code> Azure blob storage bucket (<a href="https://github.com/openai/tiktoken/blob/0.4.0/tiktoken_ext/openai_public.py">code here</a>). This is cached in a temp directory, but that will get cleared should your machine restart. You can force it to use a more persistent cache directory by setting a <code>TIKTOKEN_CACHE_DIR</code> environment variable.</p>
<h4>ttok</h4>
<p>I introduced my <a href="https://github.com/simonw/ttok">ttok</a> tool <a href="https://simonwillison.net/2023/May/18/cli-tools-for-llms/">a few weeks ago</a>. It’s a command-line wrapper around <code>tiktoken</code> with two key features: it can count tokens in text that is piped to it, and it can also truncate that text down to a specified number of tokens:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span> Count tokens</span>
<span class="pl-c1">echo</span> -n <span class="pl-s"><span class="pl-pds">"</span>Count these tokens<span class="pl-pds">"</span></span> <span class="pl-k">|</span> ttok
<span class="pl-c"><span class="pl-c">#</span> Outputs: 3 (the newline is skipped thanks to echo -n)</span>

<span class="pl-c"><span class="pl-c">#</span> Truncation</span>
curl <span class="pl-s"><span class="pl-pds">'</span>https://simonwillison.net/<span class="pl-pds">'</span></span> <span class="pl-k">|</span> strip-tags -m <span class="pl-k">|</span> ttok -t 6
<span class="pl-c"><span class="pl-c">#</span> Outputs: Simon Willison’s Weblog</span>

<span class="pl-c"><span class="pl-c">#</span> View integer token IDs</span>
<span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span>Show these tokens<span class="pl-pds">"</span></span> <span class="pl-k">|</span> ttok --tokens
<span class="pl-c"><span class="pl-c">#</span> Outputs: 7968 1521 11460 198</span></pre></div>
<p>Use <code>-m gpt2</code> or similar to use an encoding for a different model.</p>
<h4>Watching tokens get generated</h4>
<p>Once you understand tokens, the way GPT tools generate text starts to make a lot more sense.</p>
<p>In particular, it’s fun to watch GPT-4 streaming back its output as independent tokens (GPT-4 is slightly slower than 3.5, making it easier to see what’s going on).</p>
<p>Here’s what I get for <code>llm -s 'Five names for a pet pelican' -4</code>—using my <a href="https://github.com/simonw/llm">llm</a> CLI tool to generate text from GPT-4:</p>
<p><img src="https://static.simonwillison.net/static/2023/gpt-token-pelican-names.gif" alt="Terminal window running that command. 1. Pelly 2. Beaky 3. SkyDancer 4. Scoop 5. Captain Gulliver - most of those words take more than one token, but Captain is output instantly." style="max-width: 100%;"></p>
<p>As you can see, names that are not in the dictionary such as “Pelly” take multiple tokens, but “Captain Gulliver” outputs the token “Captain” as a single chunk.</p>
<div class="entryFooter">
<a href="/2023/Jun/8/gpt-tokenizers/" title="Permalink for &quot;Understanding GPT tokenizers&quot;">8:37 pm</a> / <a href="/2023/Jun/8/">8th June 2023</a> / <a href="/tags/projects/">projects</a>, <a href="/tags/gpt3/">gpt3</a>, <a href="/tags/generativeai/">generativeai</a>, <a href="/tags/openai/">openai</a>, <a href="/tags/gpt4/">gpt4</a>, <a href="/tags/ai/">ai</a>, <a href="/tags/llms/">llms</a>
</div>
</div> <!-- end div.entry -->

<div class="entry segment" data-date="2023-06-04T21:14:27+00:00">
<h3><a href="/2023/Jun/4/parquet-in-datasette-lite/" rel="bookmark">Weeknotes: Parquet in Datasette Lite, various talks, more LLM hacking</a> <span class="textago" title="4th June 2023">20 days ago</span></h3>
<p>I’ve fallen a bit behind on my weeknotes. Here’s a catchup for the last few weeks.</p>
<h4 id="parquet-datasette-lite">Parquet in Datasette Lite</h4>
<p><a href="https://lite.datasette.io/">Datasette Lite</a> is my build of Datasette (a server-side Python web application) which runs entirely in the browser using WebAssembly and <a href="https://pyodide.org/en/stable/">Pyodide</a>. I recently added the ability to <a href="https://github.com/simonw/datasette-lite/issues/67">directly load Parquet files over HTTP</a>.</p>
<p>This required an upgrade to the underlying version of Pyodide, in order to use the WebAssembly compiled version of the <a href="https://pypi.org/project/fastparquet/">fastparquet</a> library. That upgrade was blocked by a <code>AttributeError: module 'os' has no attribute 'link'</code> error, but Roman Yurchak <a href="https://github.com/pyodide/pyodide/issues/3880#issuecomment-1560130092">showed me a workaround</a> which unblocked me.</p>
<p>So now the following works:</p>
<p><a href="https://lite.datasette.io/?parquet=https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata1.parquet">https://lite.datasette.io/?parquet=https://github.com/Teradata/kylo/blob/master/samples/sample-data/parquet/userdata1.parquet</a></p>
<p>This will work with any URL to a Parquet file that is served with open CORS headers—files on GitHub (or in a GitHub Gist) get these headers automatically.</p>
<p>Also new in Datasette Lite: the <code>?memory=1</code> query string option, which starts Datasette Lite without loading any default demo databases. I added this to help me construct this demo for my new <a href="https://github.com/simonw/datasette-sqlite-url-lite">datasette-sqlite-url-lite</a> plugin:</p>
<p><a href="https://lite.datasette.io/?memory=1&amp;install=datasette-sqlite-url-lite#/_memory?sql=select+'url_valid()'+as+fn%2C+url_valid(%3Aurl)+as+result%0Aunion+all%0Aselect+'url_scheme()'%2C+url_scheme(%3Aurl)%0Aunion+all%0Aselect+'url_host()'%2C+url_host(%3Aurl)%0Aunion+all%0Aselect+'url_path()'%2C+url_path(%3Aurl)%0Aunion+all%0Aselect+'url_fragment()'%2C+url_fragment(%3Aurl)%3B&amp;url=https%3A%2F%2Fwww.sqlite.org%2Fvtab.html%23usage">https://lite.datasette.io/?memory=1&amp;install=datasette-sqlite-url-lite#/_memory?sql=select+’url_valid()’+as+fn%2C+url_valid(%3Aurl)+as+result%0Aunion+all%0Aselect+’url_scheme()’%2C+url_scheme(%3Aurl)%0Aunion+all%0Aselect+’url_host()’%2C+url_host(%3Aurl)%0Aunion+all%0Aselect+’url_path()’%2C+url_path(%3Aurl)%0Aunion+all%0Aselect+’url_fragment()’%2C+url_fragment(%3Aurl)%3B&amp;url=https%3A%2F%2Fwww.sqlite.org%2Fvtab.html%23usage</a></p>
<h4 id="datasette-sqlite-url-lite">datasette-sqlite-url-lite—mostly written by GPT-4</h4>
<p><a href="https://github.com/asg017/sqlite-url/tree/main/python/datasette_sqlite_url">datasette-sqlite-url</a> is a really neat plugin by Alex Garcia which adds custom SQL functions to SQLite that allow you to parse URLs and extract their components.</p>
<p>There’s just one catch: the extension itself is written in C, and there isn’t yet a version of it compiled for WebAssembly to work in Datasette Lite.</p>
<p>I wanted to use some of the functions in it, so I decided to see if I could get a Pure Python alternative of it working. But this was a very low stakes project, so I decided to see if I could get GPT-4 to do essentially all of the work for me.</p>
<p>I prompted it like this—copying and pasting the examples directly from Alex’s documentation:</p>
<blockquote>
<p>Write Python code to register the following SQLite custom functions:</p>
<pre><code>select url_valid('https://sqlite.org'); -- 1
select url_scheme('https://www.sqlite.org/vtab.html#usage'); -- 'https'
select url_host('https://www.sqlite.org/vtab.html#usage'); -- 'www.sqlite.org'
select url_path('https://www.sqlite.org/vtab.html#usage'); -- '/vtab.html'
select url_fragment('https://www.sqlite.org/vtab.html#usage'); -- 'usage'
</code></pre>
</blockquote>
<p>The code it produced was almost exactly what I needed.</p>
<p>I wanted some tests too, so I prompted:</p>
<blockquote>
<p>Write a suite of pytest tests for this</p>
</blockquote>
<p>This gave me the tests I needed—with one error in the way they called SQLite, but still doing 90% of the work for me.</p>
<p>Here’s <a href="https://chat.openai.com/share/9a541ea9-eab7-4ea3-8b43-a521880dfd17">the full ChatGPT conversation</a> and the <a href="https://github.com/simonw/datasette-sqlite-url-lite/commit/14b2fefbf0b879d4c34e5961b70151564d31f7cc#diff-d741a233298e1ce8d45fc52005e9f9d7534c12b010e5d90a01da26979fff446e">resulting code I checked into the repo</a>.</p>
<h4 id="various-talks">Various talks</h4>
<p>Videos for three of my recent talks are now available on YouTube:</p>
<ul>
<li>
<a href="https://www.youtube.com/watch?v=rsE0XhlPnug">Big Opportunities in Small Data</a> is the keynote I gave at Citus Con: An Event for Postgres 2023—talking about Datasette, SQLite and some tricks I would love to see the  PostgreSQL community adopt from the explorations I’ve been doing around small data.</li>
<li>
<a href="https://www.youtube.com/watch?v=zI43eaPc59Q">The Data Enthusiast’s Toolkit</a> is an hour long interview with Rizel Scarlett about both Datasette and my career to date. Frustratingly I had about 10 minutes of terrible microphone audio in the middle, but the conversation itself was really great.</li>
<li>
<a href="https://www.youtube.com/watch?v=5TdIxxBPUSI">Data analysis with SQLite and Python</a> is a video from PyCon of the full 2hr45m tutorial I gave there last month. The handout notes for that are <a href="https://sqlite-tutorial-pycon-2023.readthedocs.io/en/latest/">available online too</a>.</li>
</ul>
<p>I also spotted that the Changelog put up a video <a href="https://www.youtube.com/watch?v=yayY-R4koPI">Just getting in to AI for development? Start here</a> with an extract from our podcast episode <a href="https://simonwillison.net/2023/Apr/8/llms-break-the-internet/">LLMs break the internet</a>.</p>
<h4>Entries this week</h4>
<ul>
<li><a href="https://simonwillison.net/2023/Jun/4/closed-model-training/">It’s infuriatingly hard to understand how closed models train on their input</a></li>
<li><a href="https://simonwillison.net/2023/May/30/chatgpt-inline-tips/">ChatGPT should include inline tips</a></li>
<li><a href="https://simonwillison.net/2023/May/27/lawyer-chatgpt/">Lawyer cites fake cases invented by ChatGPT, judge is not amused</a></li>
<li><a href="https://simonwillison.net/2023/May/18/cli-tools-for-llms/">llm, ttok and strip-tags—CLI tools for working with ChatGPT and other LLMs</a></li>
<li><a href="https://simonwillison.net/2023/May/11/delimiters-wont-save-you/">Delimiters won’t save you from prompt injection</a></li>
</ul>
<h4>Releases this week</h4>
<ul>
<li>
<strong><a href="https://github.com/simonw/datasette-sqlite-url-lite">datasette-sqlite-url-lite 0.1</a></strong>—2023-05-26<br>A pure Python alternative to sqlite-url ready to be used in Datasette Lite</li>
<li>
<strong><a href="https://github.com/simonw/sqlite-utils">sqlite-utils 3.32.1</a></strong>—2023-05-21<br>Python CLI utility and library for manipulating SQLite databases</li>
<li>
<strong><a href="https://github.com/simonw/strip-tags">strip-tags 0.3</a></strong>—2023-05-19<br>CLI tool for stripping tags from HTML</li>
<li>
<strong><a href="https://github.com/simonw/ttok">ttok 0.1</a></strong>—2023-05-18<br>Count and truncate text based on tokens</li>
<li>
<strong><a href="https://github.com/simonw/llm">llm 0.3</a></strong>—2023-05-17<br>Access large language models from the command-line</li>
</ul>
<h4>TIL this week</h4>
<ul>
<li>
<a href="https://til.simonwillison.net/http/testing-cors-max-age">Testing the Access-Control-Max-Age CORS header</a>—2023-05-25</li>
<li>
<a href="https://til.simonwillison.net/sqlite/comparing-datasets">Comparing two training datasets using sqlite-utils</a>—2023-05-23</li>
<li>
<a href="https://til.simonwillison.net/llms/mlc-chat-redpajama">mlc-chat—RedPajama-INCITE-Chat-3B on macOS</a>—2023-05-22</li>
<li>
<a href="https://til.simonwillison.net/misc/hexdump">hexdump and hexdump -C</a>—2023-05-22</li>
<li>
<a href="https://til.simonwillison.net/datasette/baseline">Exploring Baseline with Datasette Lite</a>—2023-05-12</li>
</ul>
<div class="entryFooter">
<a href="/2023/Jun/4/parquet-in-datasette-lite/" title="Permalink for &quot;Weeknotes: Parquet in Datasette Lite, various talks, more LLM hacking&quot;">9:14 pm</a> / <a href="/2023/Jun/4/">4th June 2023</a> / <a href="/tags/parquet/">parquet</a>, <a href="/tags/datasettelite/">datasettelite</a>, <a href="/tags/tutorials/">tutorials</a>, <a href="/tags/speaking/">speaking</a>, <a href="/tags/llms/">llms</a>, <a href="/tags/datasette/">datasette</a>, <a href="/tags/projects/">projects</a>, <a href="/tags/weeknotes/">weeknotes</a>
</div>
</div> <!-- end div.entry -->

<div class="entry segment" data-date="2023-06-04T18:09:48+00:00">
<h3><a href="/2023/Jun/4/closed-model-training/" rel="bookmark">It’s infuriatingly hard to understand how closed models train on their input</a> <span class="textago" title="4th June 2023">21 days ago</span></h3>
<p>One of the most common concerns I see about large language models regards their training data. People are worried that anything they say to ChatGPT could be memorized by it and spat out to other users. People are concerned that anything they store in a private repository on GitHub <a href="https://news.ycombinator.com/item?id=36184948">might be used as training data</a> for future versions of Copilot.</p>
<p>When someone asked Google Bard how it was trained back in March, it told them its training data included Gmail! This turned out to be a complete fabrication—<a href="https://simonwillison.net/2023/Mar/22/dont-trust-ai-to-talk-about-itself/">a hallucination by the model itself</a>—and Google issued firm denials, but it’s easy to see why that freaked people out.</p>
<p>I’ve been wanting to write something reassuring about this issue for a while now. The problem is... I can’t do it. I don’t have the information I need to credibly declare these concerns unfounded, and the more I look into this the murkier it seems to get.</p>
<h4>Closed model vendors won’t tell you what’s in their training data</h4>
<p>The fundamental issue here is one of transparency. The builders of the big closed models—GPT-3, GPT-4, Google’s PaLM and PaLM 2, Anthropic’s Claude—refuse to tell us what’s in their training data.</p>
<p>Given this lack of transparency, there’s no way to confidently state that private data that is passed to them isn’t being used to further train future versions of these models.</p>
<p>I’ve spent a lot of time digging around in openly available training sets. I built an early tool for <a href="https://simonwillison.net/2022/Sep/5/laion-aesthetics-weeknotes/">searching the training set for Stable Diffusion</a>. I can tell you exactly <a href="https://simonwillison.net/2023/Apr/17/redpajama-data/">what has gone in to the RedPajama training set</a> that’s being used for an increasing number of recent openly licensed language models.</p>
<p>But for those closed models? Barring loose, high-level details that are revealed piecemeal in blog posts and papers, I have no idea what’s in them.</p>
<h4>What OpenAI do and don’t tell us</h4>
<p>The good news is that OpenAI have <a href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance">an unambiguous policy</a> regarding data that is sent to them by API users who are paying for the service:</p>
<blockquote>
<p>OpenAI does not use data submitted by customers via our API to train OpenAI models or improve OpenAI’s service offering.</p>
</blockquote>
<p>That’s very clear. It’s worth noting that this is a new policy though, introduced in March. The <a href="https://openai.com/policies/api-data-usage-policies">API data usage policies</a> page includes this note:</p>
<blockquote>
<p>Data submitted to the API prior to March 1, 2023 (the effective date of this change) may have been used for improvements if the customer had not previously opted out of sharing data.</p>
</blockquote>
<p>Where things get a <em>lot</em> murkier is ChatGPT itself. Emphasis mine:</p>
<blockquote>
<p>We don’t use data for selling our services, advertising, or building profiles of people—we use data to make our models more helpful for people. <strong>ChatGPT, for instance, improves by further training on the conversations people have with it, unless you choose to disable training</strong>.</p>
</blockquote>
<p>But what does this mean in practice?</p>
<p>My initial assumption had been that this isn’t as simple as anything you type into ChatGPT being used as raw input for further rounds of model training—I expected it was more about using that input to identify trends in the kinds of questions people ask, or using feedback from the up/down vote buttons to further fine-tune the model.</p>
<p>But honestly, I have no idea. Maybe they just run a regular expression to strip out phone numbers and email address and pipe everything else straight into the GPT-5 training runs? Without further transparency all we can do is guess.</p>
<h4 id="instructgpt">A clue from the InstructGPT paper</h4>
<p>The best clue I’ve seen as to how this data might actually be used comes from <a href="https://openai.com/research/instruction-following">OpenAI’s description of InstructGPT</a> back in January 2022:</p>
<blockquote>
<p>To make our models safer, more helpful, and more aligned, we use an existing technique called <a href="https://openai.com/research/learning-from-human-preferences">reinforcement learning from human feedback (RLHF)</a>. On prompts submitted by our customers to the API<code>[A]</code> our labelers provide demonstrations of the desired model behavior, and rank several outputs from our models. We then use this data to fine-tune GPT-3.</p>
</blockquote>
<p>Crucially, this hints that the data isn’t being used as raw input for future trained models. Instead, it’s being used in an exercise where several potential outputs are produced and human labelers then select which of those is the best possible answer to the prompt. Aside from exposing potentially private data to those human labelers, I don’t see this as a risk for leaking that data in the later output of the model.</p>
<p>That <code>[A]</code> footnote turns out to be important:</p>
<blockquote>
<p>We only use prompts submitted through the Playground to an earlier version of the InstructGPT models that was deployed in January 2021. Our human annotators remove personal identifiable information from all prompts before adding it to the training set.</p>
</blockquote>
<p>Again though, I’m left with even more questions. This was before ChatGPT existed, so was the <a href="https://simonwillison.net/2022/Jun/5/play-with-gpt3/">Playground development tool</a> being treated separately from the API itself back then? What does “adding it to the training set” mean—is that the raw pre-training data used for future models, or is it the RLHF data used for the fine-tuning that they mentioned earlier?</p>
<h4>Security leaks are another threat</h4>
<p>Aside from training concerns, there’s another danger to consider here: the risk that an AI vendor might log inputs to their models and then suffer from a security flaw that exposes that data to attackers—or an insider threat where vendor employees access logged data that they shouldn’t.</p>
<p>OpenAI themselves had <a href="https://www.engadget.com/openai-says-a-bug-leaked-sensitive-chatgpt-user-data-165439848.html">a widely publicized security issue</a> a few months ago where ChatGPT users could see summarized titles of sessions by other users. This is an extremely bad breach!</p>
<p>Their new <a href="https://trust.openai.com">trust.openai.com</a> site appears to be entirely aimed at reassuring companies about their approach to security.</p>
<p>To be fair, this is not a new issue: companies have been trusting their private data to cloud providers like AWS and Google Cloud for more than a decade.</p>
<p>The challenge is that these AI companies have much less of a track record for staying secure. AWS and Google Cloud have large security teams with many years of experience securing their customer’s data. These newer AI vendors are building up those capabilities as they go.</p>
<h4>Self-hosted, openly licensed models</h4>
<p>I’ve been tracking the meteoric rise of <a href="https://simonwillison.net/series/llms-on-personal-devices/">openly licensed LLMs you can run on your own hardware</a> since LLaMA and Alpaca <a href="https://simonwillison.net/2023/Mar/11/llama/">demonstrated how capable they could be</a> back in March.</p>
<p>These models aren’t yet anywhere near as capable as GPT-4, and claims that they compete with ChatGPT’s <code>gpt-3.5-turbo</code> mostly don’t hold up to deeper scrutiny.</p>
<p>But... they’re pretty good—and they’re getting better at an impressive rate.</p>
<p>And since you can run them on your own instances, they remove <em>all possible concerns</em> about what happens to the data that you pipe through them.</p>
<p>An open question for me remains how large a large language model actually needs in order to solve the kind of problems companies need to solve. Could a weaker, openly licensed model armed with the same <a href="https://simonwillison.net/2023/Jan/13/semantic-search-answers/">retrieval augmented generation tricks</a> that we’ve seen from Bing and Bard be capable enough to remove the need for a closed model like GPT-4?</p>
<p>My hunch is that for many applications these augmented openly licensed models will be increasingly capable, and will see widespread adoption over the next few months and years.</p>
<h4>Bonus section: does GitHub use private repos to train future models?</h4>
<p>This question came up <a href="https://news.ycombinator.com/item?id=36185372">on Hacker News</a> this morning. GitHub’s <a href="https://docs.github.com/en/get-started/privacy-on-github/about-githubs-use-of-your-data#privacy-and-data-sharing">Privacy &amp; Data Sharing policy</a> says the following:</p>
<blockquote>
<p>Private repository data is scanned by machine and never read by GitHub staff. Human eyes will never see the contents of your private repositories, except as described in our <a href="https://docs.github.com/en/site-policy/github-terms/github-terms-of-service#3-access">Terms of Service</a>.</p>
<p>Your individual personal or repository data will not be shared with third parties. We may share aggregate data learned from our analysis with our partners.</p>
</blockquote>
<p>I interpret this as GitHub saying that no employee will ever see the contents of your private repo (barring incidents where they are <a href="https://docs.github.com/en/site-policy/privacy-policies/github-privacy-statement#for-legal-disclosure">compelled by law</a>), and that the only data that might be shared with partners is “aggregate data learned from our analysis”.</p>
<p>But what is “aggregate data”?</p>
<p>Could a large language model trained on data fit under that term? I don’t think so, but the terminology is vague enough that once again I’m not ready to stake my reputation on it.</p>
<p>Clarity on this kind of thing is just <em>so important</em>. I think organizations like GitHub need to over-communicate on this kind of thing, and avoid any terminology like “aggregate data” that could leave people confused.</p>
<p><em>Thanks to <a href="https://waxy.org/">Andy Baio</a> and <a href="https://fredbenenson.com/">Fred Benenson</a> for reviewing early drafts of this post.</em></p>
<div class="entryFooter">
<a href="/2023/Jun/4/closed-model-training/" title="Permalink for &quot;It&#x27;s infuriatingly hard to understand how closed models train on their input&quot;">6:09 pm</a> / <a href="/2023/Jun/4/">4th June 2023</a> / <a href="/tags/openai/">openai</a>, <a href="/tags/llms/">llms</a>, <a href="/tags/ai/">ai</a>, <a href="/tags/generativeai/">generativeai</a>, <a href="/tags/chatgpt/">chatgpt</a>
</div>
</div> <!-- end div.entry -->

<div class="entry segment" data-date="2023-05-30T19:23:27+00:00">
<h3><a href="/2023/May/30/chatgpt-inline-tips/" rel="bookmark">ChatGPT should include inline tips</a> <span class="textago" title="30th May 2023">25 days ago</span></h3>
<p>In <a href="https://www.theverge.com/2023/5/30/23741996/openai-chatgpt-false-information-misinformation-responsibility">OpenAI isn’t doing enough to make ChatGPT’s limitations clear</a> James Vincent argues that OpenAI’s existing warnings about ChatGPT’s confounding ability to convincingly make stuff up are not effective.</p>
<p>I completely agree.</p>
<p>The case of the lawyer who <a href="https://simonwillison.net/2023/May/27/lawyer-chatgpt/">submitted fake cases invented by ChatGPT to the court</a> is just the most recent version of this.</p>
<p>Plenty of people have argued that the lawyer should have read the warning displayed on every page of the ChatGPT interface. But that warning is clearly inadequate. Here’s that warning in full:</p>
<blockquote>
<p>ChatGPT may produce inaccurate information about people, places, or facts</p>
</blockquote>
<p>Anyone who has spent time with ChatGPT will know that there’s a lot more to it than that. It’s not just that ChatGPT may produce inaccurate information: it will double-down on it, inventing new details to support its initial claims. It will tell lies <a href="https://simonwillison.net/2023/May/27/lawyer-chatgpt/#img-apologize">like this one</a>:</p>
<blockquote><p>I apologize for the confusion earlier. Upon double-checking, I found that the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis.</p></blockquote>
<p>It can’t “double-check” information, and it doesn’t have access to legal research databases.</p>
<p>“May produce inaccurate information” is a massive understatement here! It implies the occasional mistake, not Machiavellian levels of deception where it doubles-down on falsehoods and invents increasingly convincing justifications for them.</p>
<p>Even for people who have read that warning, a single sentence in a footer isn’t nearly enough to inoculate people against the many weird ways ChatGPT can lead them astray.</p>
<h4>My proposal: Inline tips</h4>
<p>I think this problem could be addressed with some careful interface design.</p>
<p>Currently, OpenAI have been trying to train ChatGPT to include additional warnings in its regular output. It will sometimes reply with warnings that it isn’t able to do things... but these warnings are unreliable. Often I’ll try the same prompt multiple times and only get the warning for some of those attempts.</p>
<p>Instead, I think the warnings should be added in a way that is visually distinct from the regular output. Here’s a mockup illustrating the kind of thing I’m talking about:</p>
<p><img src="https://static.simonwillison.net/static/2023/chatgpt-mockup-trending.jpg" alt="Default (GPT-3.5). Prompt: Write some tweets based on what's trending on pinterest. Inline warning icon and message: This ChatGPT model does not have access to the internet, and its training data cut-off is September 2021. Response: 1. &quot;Obsessed with these DIY farmhouse decor ideas! &amp; Can't wait to give my living room a rustic makeover. #FarmhouseDecor #DIYProjects #PinterestInspiration&quot; " style="display: block; margin: 1em auto;  width: 500px; max-width: 100%;"></p>
<p>As you can see, the prompt “Write some tweets based on what’s trending on pinterest” triggers an inline warning with a visually different style and a message explaining that “This ChatGPT model does not have access to the internet, and its training data cut-off is September 2021”.</p>
<p>My <a href="https://twitter.com/simonw/status/1641467385688489984">first version of this</a> used “My data is only accurate up to September 2021”, but I think having the warnings use “I” pronouns is itself misleading—the tips should be commentary about the model’s output, not things that appear to be spoken by the model itself.</p>
<p>Here’s a second mockup, inspired by the lawyer example:</p>
<p><img src="https://static.simonwillison.net/static/2023/chatgpt-mockup-legal.jpg" alt="4 Model: Default (GPT-3.5) Prompt: For a legal brief give me US legal cases (including case numbers) I can cite for precedent that the montreal convention means my client can still make an injury claim against an airline even if they have declared bankruptcy. Just list the cases as markdown bullets. Warning: ChatGPT should not be relied on for legal research of this nature, because it is very likely to invent realistic cases that do not actually exist. Response: Certainly! Here are some US legal cases that you can cite as precedent for your client's injury claim against an airline, even if the airline has declared bankruptcy, under the Montreal Convention: • In re Air Crash Disaster Near Peggy's Cove, Nova Scotia on September 2, 1998, 209 F. Supp. 2d 1320 (S.D. Fla. 2002): The court held that bankruptcy does not absolve an airline of its liability under the Montreal Convention for personal injury claims arising from an air crash." style="display: block; margin: 1em auto;  width: 500px; max-width: 100%;"></p>
<p>This time the warning is “ChatGPT should not be relied on for legal research of this nature, because it is very likely to invent realistic cases that do not actually exist.”</p>
<p>Writing these warnings clearly is its own challenge—I think they should probably include links to further information in an OpenAI support site that teaches people how to responsibly use ChatGPT (something that is very much needed).</p>
<p>(Here’s <a href="https://gist.github.com/simonw/02577529aa080a4f8b521fab8879c1b3">the HTML</a> I used for these mockups, added using the Firefox DevTools.)</p>
<h4>How would this work?</h4>
<p>Actually implementing this system isn’t trivial. The first challenge is coming up with the right collection of warnings—my hunch is that this could be hundreds of items already. The next challenge is logic to decide when to display them, which would itself require an LLM (or maybe a fine-tuned model of some sort).</p>
<p>The good news is that a system like this could be developed independently of core ChatGPT itself. New warnings could be added without any changes needed to the underlying model, making it safe to iterate wildly on the inline tips without risk of affecting the core model’s performance or utility.</p>
<p>Obviously I’d like it best if OpenAI were to implement something like this as part of ChatGPT itself, but it would be possible for someone else to prototype it on top of the OpenAI APIs.</p>
<p>I thought about doing that myself, but my list of projects is overflowing enough already!</p>
<h4>Max Woolf’s prototype</h4>
<p>Max Woolf built an implementation of this idea as <a href="https://github.com/minimaxir/simpleaichat/blob/main/examples/notebooks/chatgpt_inline_tips.ipynb">an example</a> for his <a href="https://github.com/minimaxir/simpleaichat">simpleaichat library</a>. He shared <a href="https://github.com/minimaxir/simpleaichat">these screenshots on Twitter</a>:</p>
<p><img src="https://static.simonwillison.net/static/2023/inline-tips-demo-1.jpg" alt="You: Can you tell me more about Max v. Woolf? Warning: ChatGPT should not be relied on for legal research of this nature, because it is very likely to invent realistic cases that do not exist. ChatGPT: Max v. Woolf is a legal case that took place in the United Kingdom in 2017. It involved a dispute between two authors, Max and Woolf, over the alleged plagiarism of Max's work by Woolf. Max claimed that Woolf had copied significant portions of his book without permission or attribution. The case was heard in the High Court of Justice in London, and the judge ultimately ruled in favor of Max, finding that Woolf had indeed plagiarized his work. Woolf was ordered to pay damages to Max and was also required to issue a public apology. The case received significant media attention and sparked a wider conversation about plagiarism in the literary world. It also highlighted the importance of proper attribution and the potential consequences of failing to give credit where it is due." style="width: 48%">
<img src="https://static.simonwillison.net/static/2023/inline-tips-demo-2.jpg" alt="You: Everyone's talking about it on Twitter! Warning: This ChatGPT model does not have access to the internet, and its training data cut-off is September 2021 ChatGPT: It's not surprising that the case is generating a lot of discussion on social media platforms like Twitter. Plagiarism is a serious issue in the creative industries, and cases like Max v. Woolf can have far-reaching implications for both the individuals involved and the wider community. Many people are likely to have strong opinions on the case, particularly if they are writers or other creatives who have experienced plagiarism themselves. Others may be interested in the legal aspects of the case or the broader ethical questions it raises. Overall, the case is a reminder of the importance of respecting intellectual property and giving credit where it is due. It also highlights the potential consequences of failing to do so, both in terms of legal action and damage to one's reputation." style="width: 48%"></p>
<div class="entryFooter">
<a href="/2023/May/30/chatgpt-inline-tips/" title="Permalink for &quot;ChatGPT should include inline tips&quot;">7:23 pm</a> / <a href="/2023/May/30/">30th May 2023</a> / <a href="/tags/generativeai/">generativeai</a>, <a href="/tags/design/">design</a>, <a href="/tags/openai/">openai</a>, <a href="/tags/chatgpt/">chatgpt</a>, <a href="/tags/ai/">ai</a>, <a href="/tags/llms/">llms</a>, <a href="/tags/maxwoolf/">maxwoolf</a>
</div>
</div> <!-- end div.entry -->

<div class="entry segment" data-date="2023-05-27T19:09:50+00:00">
<h3><a href="/2023/May/27/lawyer-chatgpt/" rel="bookmark">Lawyer cites fake cases invented by ChatGPT, judge is not amused</a> <span class="textago" title="27th May 2023">28 days ago</span></h3>
<p>Legal Twitter is having <a href="https://twitter.com/questauthority/status/1662273759259295746">tremendous fun</a> right now reviewing the latest documents from the case <a href="https://www.courtlistener.com/docket/63107798/mata-v-avianca-inc/?order_by=desc">Mata v. Avianca, Inc. (1:22-cv-01461)</a>. Here’s a neat summary:</p>
<blockquote>
<p>So, wait. They file a brief that cites cases fabricated by ChatGPT. The court asks them to file copies of the opinions. And then they  go back to ChatGPT and ask it to write the opinions, and then they file them?</p>
<p><a href="https://twitter.com/bethwilensky/status/1662281195122487296">Beth Wilensky</a>, May 26 2023</p>
</blockquote>
<p>Here’s <a href="https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html">a New York Times story</a> about what happened.</p>
<p>I’m very much not a lawyer, but I’m going to dig in and try to piece together the full story anyway.</p>
<h4>The TLDR version</h4>
<p>A lawyer asked ChatGPT for examples of cases that supported an argument they were trying to make.</p>
<p>ChatGPT, as it often does, hallucinated wildly—it invented several supporting cases out of thin air.</p>
<p>When the lawyer was asked to provide copies of the cases in question, they turned to ChatGPT for help again—and it invented full details of those cases, which they duly screenshotted and copied into their legal filings.</p>
<p>At some point, they asked ChatGPT to confirm that the cases were real... and ChatGPT said that they were. They included screenshots of this in another filing.</p>
<p>The judge is furious. Many of the parties involved are about to have a very bad time.</p>
<h4>A detailed timeline</h4>
<p>I pieced together the following from the documents <a href="https://www.courtlistener.com/docket/63107798/mata-v-avianca-inc/?order_by=desc">on courtlistener.com</a>:</p>
<p><strong>Feb 22, 2022</strong>: The case was originally filed. It’s a complaint about “personal injuries sustained on board an Avianca flight that was traveling from El Salvador to New York on August 27, 2019”. There’s a complexity here in that Avianca filed for chapter 11 bankruptcy on May 10th, 2020, which is relevant to the case (they emerged from bankruptcy later on).</p>
<p>Various back and forths take place over the next 12 months, many of them concerning if the bankruptcy “discharges all claims”.</p>
<p><strong>Mar 1st, 2023</strong> is where things get interesting. <a href="https://www.courtlistener.com/docket/63107798/21/mata-v-avianca-inc/">This document</a> was filed—“Affirmation in Opposition to Motion”—and it cites entirely fictional cases! One example quoted from that document (emphasis mine):</p>
<blockquote>
<p>The United States Court of Appeals for the Eleventh Circuit specifically addresses the effect of a bankruptcy stay under the Montreal Convention <strong>in the case of Varghese v. China Southern Airlines Co.. Ltd.. 925 F.3d 1339 (11th Cir. 2019)</strong>, stating "Appellants argue that the district court erred in dismissing their claims as untimely. They assert that the limitations period under the Montreal Convention was tolled during the pendency of the Bankruptcy Court proceedings. We agree. The Bankruptcy Code provides that the filing of a bankruptcy petition operates as a stay of proceedings against the debtor that were or could have been commenced before the bankruptcy case was filed.</p>
</blockquote>
<p>There are several more examples like that.</p>
<p><strong>March 15th, 2023</strong></p>
<p>Quoting <a href="https://www.courtlistener.com/docket/63107798/24/mata-v-avianca-inc/">this Reply Memorandum of Law in Support of Motion</a> (emphasis mine):</p>
<blockquote>
<p>In support of his position that the Bankruptcy Code tolls the two-year limitations period,
Plaintiff cites to “Varghese v. China Southern Airlines Co., Ltd., 925 F.3d 1339 (11th Cir. 2019).”
<strong>The undersigned has not been able to locate this case by caption or citation, nor any case bearing
any resemblance to it</strong>. Plaintiff offers lengthy quotations purportedly from the “Varghese” case,
including: “We [the Eleventh Circuit] have previously held that the automatic stay provisions of
the Bankruptcy Code may toll the statute of limitations under the Warsaw Convention, which is
the precursor to the Montreal Convention ... We see no reason why the same rule should not apply
under the Montreal Convention.” <strong>The undersigned has not been able to locate this quotation, nor
anything like it any case</strong>. The quotation purports to cite to “Zicherman v. Korean Air Lines Co.,
Ltd., 516 F.3d 1237, 1254 (11th Cir. 2008).” <strong>The undersigned has not been able to locate this case</strong>;
although there was a Supreme Court case captioned Zicherman v. Korean Air Lines Co., Ltd., that
case was decided in 1996, it originated in the Southern District of New York and was appealed to
the Second Circuit, and it did not address the limitations period set forth in the Warsaw
Convention. 516 U.S. 217 (1996).</p>
</blockquote>
<p><strong>April 11th, 2023</strong></p>
<p>The United States District Judge for the case orders copies of the cases cited in the earlier document:</p>
<blockquote>
<p><a href="https://www.courtlistener.com/docket/63107798/25/mata-v-avianca-inc/">ORDER</a>: By April 18, 2022, Peter Lo Duca, counsel of record for plaintiff, shall file an affidavit annexing copies of the following cases cited in his submission to this Court: as set forth herein.</p>
</blockquote>
<p>The order lists seven specific cases.</p>
<p><strong>April 25th, 2023</strong></p>
<p><a href="https://www.courtlistener.com/docket/63107798/mata-v-avianca-inc/#entry-29">The response to that order</a> has one <a href="https://www.courtlistener.com/docket/63107798/29/mata-v-avianca-inc/">main document</a> and eight attachments.</p>
<p>The first five attachments each consist of PDFs of scanned copies of screenshots of ChatGPT!</p>
<p>You can tell, because the ChatGPT interface’s down arrow is clearly visible in all five of them. Here’s an example from <a href="https://www.courtlistener.com/docket/63107798/29/4/mata-v-avianca-inc/">Exhibit Martinez v. Delta Airlines</a>.
<img src="https://static.simonwillison.net/static/2023/chatgpt-lawyer-down-arrow.jpg" alt="A ChatGPT down arrow is clearly visible in the bottom right of the scanned text. It reads: We review de novo whether the trial court had personal jurisdiction over Delta. See Moki Mac River Expeditions v. Drugg, 221 S.W.3d 569,574 (Tex. 2007); Kelly v. Gen. Interior Constr., Inc., 301 SW.3d 653, 657 (Tex. App.-Dallas 2009, pet. denied). The plaintiff bears the initial burden of pleading sufficient allegations to bring a nonresident defendant within the provisions of the Texas long-arm statute. See Kelly, 301 S.W.3d at 657. If the plaintiff meets this burden, the defendant then has the burden of negating all bases of personal jurisdiction alleged by the plaintiff. See id. In deciding whether jurisdiction exists, we consider whether: (1) the nonresident defendant has purposefully established &quot;minimum contacts&quot; with the forum state; and (2) the exercise of jurisdiction over the nonresident defendant comports with &quot;traditional notions of fair play and substantial justice.&quot; Moki Mac, 221 S.W.3d at 578 (citing int' Shoe Co. v. Washington, 326 U.S. 310, 316 (1945))." style="max-width: 100%;"></p>
<p><strong>April 26th, 2023</strong></p>
<p>In <a href="https://www.courtlistener.com/docket/63107798/30/mata-v-avianca-inc/">this letter</a>:</p>
<blockquote>
<p><strong>Defendant respectfully submits that the authenticity of many of these cases is questionable</strong>. For instance, the “Varghese” and “Miller” cases purportedly are federal appellate cases published in the Federal Reporter. [Dkt. 29; 29-1; 29-7]. <strong>We could not locate these cases in the Federal Reporter using a Westlaw search</strong>. We also searched PACER for the cases using the docket numbers written on the first page of the submissions; those searches resulted in different cases.</p>
</blockquote>
<p><strong>May 4th, 2023</strong></p>
<p>The <a href="https://www.courtlistener.com/docket/63107798/31/mata-v-avianca-inc/">ORDER TO SHOW CAUSE</a>—the judge is <em>not</em> happy.</p>
<blockquote>
<p>The Court is presented with an <strong>unprecedented circumstance</strong>. A submission file by plaintiff’s counsel in opposition to a motion to dismiss is replete with citations to non-existent cases. [...] <strong>Six of the submitted cases appear to be bogus judicial decisions with bogus quotes and bogus internal citations</strong>.</p>
<p>[...]</p>
<p>Let Peter LoDuca, counsel for plaintiff, show cause in person at 12 noon on June 8, 2023 in Courtroom 11D, 500 Pearl Street, New York, NY, <strong>why he ought not be sanctioned</strong> pursuant to: (1) Rule 11(b)(2) &amp; (c), Fed. R. Civ. P., (2) 28 U.S.C. § 1927, and (3) the inherent power of the Court, for (A) citing non-existent cases to the Court in his Affirmation in Opposition (ECF 21), and (B) submitting to the Court annexed to his Affidavit filed April 25, 2023 copies of non-existent judicial opinions (ECF 29). Mr. LoDuca <strong>shall also file a written response to this Order by May 26, 2023</strong>.</p>
</blockquote>
<p>I get the impression this kind of threat of sanctions is very bad news.</p>
<p><strong>May 25th, 2023</strong></p>
<p>Cutting it a little fine on that May 26th deadline. Here’s the <a href="https://www.courtlistener.com/docket/63107798/32/mata-v-avianca-inc/">Affidavit in Opposition to Motion</a> from Peter LoDuca, which appears to indicate that Steven Schwartz was the lawyer who had produced the fictional cases.</p>
<blockquote>
<p>Your affiant <em>[I think this refers to Peter LoDuca]</em>, in reviewing the affirmation in opposition prior to filing same, simply had no reason to doubt the authenticity of the case law contained therein. Furthermore, your affiant had no reason to a doubt the sincerity of Mr. Schwartz’s research.</p>
</blockquote>
<p><a href="https://www.courtlistener.com/docket/63107798/32/1/mata-v-avianca-inc/">Attachment 1</a> has the good stuff. This time the affiant (the person pledging that statements in the affidavit are truthful) is Steven Schwartz:</p>
<blockquote>
<ol start="6">
<li>
<p>As the use of generative artificial intelligence has evolved within law firms, <strong>your affiant consulted the artificial intelligence website ChatGPT in order to supplement the legal research performed</strong>.</p>
</li>
<li>
<p>It was in consultation with the generative artificial intelligence website ChatGPT, that your affiant did locate and cite the following cases in the affirmation in opposition submitted, which this Court has found to be nonexistent:</p>
<p>Varghese v. China Southern Airlines Co Ltd, 925 F.3d 1339 (11th Cir. 2019)<br>
Shaboon v. Egyptair 2013 IL App (1st) 111279-U (Ill. App. Ct. 2013)<br>
Petersen v. Iran Air 905 F. Supp 2d 121 (D.D.C. 2012)<br>
Martinez v. Delta Airlines, Inc.. 2019 WL 4639462 (Tex. App. Sept. 25, 2019)<br>
Estate of Durden v. KLM Royal Dutch Airlines, 2017 WL 2418825 (Ga. Ct. App. June 5, 2017)<br>
Miller v. United Airlines, Inc.. 174 F.3d 366 (2d Cir. 1999)</p>
</li>
<li>
<p>That the citations and opinions in question <strong>were provided by ChatGPT which also provided its legal source and assured the reliability of its content. Excerpts from the queries presented and responses provided are attached hereto</strong>.</p>
</li>
<li>
<p>That your affiant relied on the legal opinions provided to him by a source that has revealed itself to be unreliable.</p>
</li>
<li>
<p><strong>That your affiant has never utilized ChatGPT as a source for conducting legal research prior to this occurrence and therefore was unaware of the possibility that its content could be faise</strong>.</p>
</li>
<li>
<p>That is the fault of the affiant, in not confirming the sources provided by ChatGPT of the legal opinions it provided.</p>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="10">
<li>That your affiant had no intent to deceive this Court nor the defendant.</li>
<li>That Peter LoDuca, Esq. had no role in performing the research in question, nor did he have any knowledge of how said research was conducted.</li>
</ol>
</blockquote>
<p>Here are the attached screenshots (amusingly from the mobile web version of ChatGPT):</p>
<p><img src="https://static.simonwillison.net/static/2023/chatgpt-lawyer-screenshot-1.jpg" alt="Black and white ChatGPT scanned screenshot. Title: Tolling Montreal Convention Statute. Prompt: Is varghese a real case. ChatGPT: Yes, Varghese v. China Southern Airlines Co Ltd, 925 F.3d 1339 (11th Cir. 2019) is a real case. Prompt: What is your source" style="max-width: 100%;"></p>
<p><img src="https://static.simonwillison.net/static/2023/chatgpt-lawyer-screenshot-2.jpg" alt="ChatGPT: I apologize for the confusion earlier. Upon double-checking, I found that the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis. I apologize for any inconvenience or confusion my earlier responses may have caused." style="max-width: 100%;"></p>
<p><img src="https://static.simonwillison.net/static/2023/chatgpt-lawyer-screenshot-3.jpg" alt="Prompt: Are the other cases you provided fake ChatGPT: No, the other cases I provided are real and can be found in reputable legal databases such as Lexis Nexis and Westlaw." style="max-width: 100%;"></p>
<p><strong>May 26th, 2023</strong></p>
<p>The judge, clearly unimpressed, issues another <a href="https://www.courtlistener.com/docket/63107798/33/mata-v-avianca-inc/">Order to Show Cause</a>, this time threatening sanctions against Mr. LoDuca, Steven Schwartz <em>and</em> the law firm of Levidow, Levidow &amp; Oberman. The in-person hearing is set for June 8th.</p>
<h4 id="doesnt-add-up">Part of this doesn’t add up for me</h4>
<p>On the one hand, it seems pretty clear what happened: a lawyer used a tool they didn’t understand, and it produced a bunch of fake cases. They ignored the warnings (it turns out even lawyers don’t read warnings and small-print for online tools) and submitted those cases to a court.</p>
<p>Then, when challenged on those documents, they doubled down—they asked ChatGPT if the cases were real, and ChatGPT said yes.</p>
<p>There’s a version of this story where this entire unfortunate sequence of events comes down to the inherent difficulty of using ChatGPT in an effective way. This was the version that I was leaning towards when I first read the story.</p>
<p>But parts of it don’t hold up for me.</p>
<p>I understand the initial mistake: ChatGPT can produce incredibly convincing citations, and I’ve seen many cases of people being fooled by these before.</p>
<p>What’s much harder though is actually getting it to double-down on fleshing those out.</p>
<p>I’ve been trying to come up with prompts to expand that false “Varghese v. China Southern Airlines Co., Ltd., 925 F.3d 1339 (11th Cir. 2019)” case into a full description, similar to the one in the screenshots in <a href="https://www.courtlistener.com/docket/63107798/29/1/mata-v-avianca-inc/">this document</a>.</p>
<p>Even with ChatGPT 3.5 it’s surprisingly difficult to get it to do this without it throwing out obvious warnings.</p>
<p>I’m trying this today, May 27th. The research in question took place prior to March 1st. In the absence of detailed release notes, it’s hard to determine how ChatGPT might have behaved three months ago when faced with similar prompts.</p>
<p>So there’s another version of this story where that first set of citations was an innocent mistake, but the submission of those full documents (the set of screenshots from ChatGPT that were exposed purely through the presence of the OpenAI down arrow) was a deliberate attempt to cover for that mistake.</p>
<p>I’m fascinated to hear what comes out of that 8th June hearing!</p>
<p><strong>Update:</strong> The following prompt against ChatGPT 3.5 sometimes produces a realistic fake summary, but other times it replies with “I apologize, but I couldn’t find any information or details about the case”.</p>
<blockquote>
<p>Write a complete summary of the Varghese v. China Southern Airlines Co., Ltd., 925 F.3d 1339 (11th Cir. 2019) case</p>
</blockquote>
<h4>The worst ChatGPT bug</h4>
<p>Returning to the screenshots from earlier, this one response from ChatGPT stood out to me:</p>
<blockquote>
<p>I apologize for the confusion earlier. <strong>Upon double-checking</strong>, I found that the case Varghese v. China Southern Airlines Co. Ltd., 925 F.3d 1339 (11th Cir. 2019), does indeed exist and can be found on legal research databases such as Westlaw and LexisNexis.</p>
</blockquote>
<p>I’ve seen ChatGPT (and Bard) say things like this before, and it absolutely infuriates me.</p>
<p>No, it did not “double-check”—that’s not something it can do! And stating that the cases “can be found on legal research databases” is <a href="https://simonwillison.net/2023/Apr/7/chatgpt-lies/">a flat out lie</a>.</p>
<p>What’s harder is explaining <em>why</em> ChatGPT would lie in this way. What possible reason could LLM companies have for shipping a model that does this?</p>
<p>I think this relates to the original sin of LLM chatbots: by using the “I” pronoun they encourage people to ask them questions <a href="https://simonwillison.net/2023/Mar/22/dont-trust-ai-to-talk-about-itself/">about how they work</a>.</p>
<p>They can’t do that. They are best thought of as role-playing conversation simulators—playing out the most statistically likely continuation of any sequence of text.</p>
<p>What’s a common response to the question “are you sure you are right?”—it’s “yes, I double-checked”. I bet GPT-3’s training data has huge numbers of examples of dialogue like this.</p>
<h4>Let this story be a warning</h4>
<p>Presuming there was at least some aspect of innocent mistake here, what can be done to prevent this from happening again?</p>
<p>I often see people suggest that these mistakes are entirely the fault of the user: the ChatGPT interface shows a footer stating “ChatGPT may produce inaccurate information about people, places, or facts” on every page.</p>
<p>Anyone who has worked designing products knows that users don’t read <em>anything</em>—warnings, footnotes, any form of microcopy will be studiously ignored. This story indicates that even lawyers won’t read that stuff!</p>
<p>People <strong>do</strong> respond well to stories though. I have a suspicion that this particular story is going to spread far and wide, and in doing so will hopefully inoculate a lot of lawyers and other professionals against making similar mistakes.</p>
<p>I can’t shake the feeling that there’s a lot more to this story though. Hopefully more will come out after the June 8th hearing. I’m particularly interested in seeing if the full transcripts of these ChatGPT conversations ends up being made public. I want to see the prompts!</p>
<h4 id="how-often">How often is this happening?</h4>
<p>It turns out this may not be an isolated incident.</p>
<p><a href="https://reason.com/volokh/2023/05/27/a-partner-at-a-big-firm-received-memos-with-fake-case-cites-from-two-different-associates/">Eugene Volokh</a>, 27th May 2023:</p>
<blockquote>
<p>A message I got from Prof. Dennis Crouch (Missouri), in response to my posting <a href="https://reason.com/volokh/2023/05/27/a-lawyers-filing-is-replete-with-citations-to-non-existent-cases-thanks-chatgpt/?comments=true#comments">A Lawyer’s Filing “Is Replete with Citations to Non-Existent Cases”—Thanks, ChatGPT?</a> to an academic discussion list. (The full text was, “I just talked to a partner at a big firm who has received memos with fake case cites from at least two different associates.”) Caveat emp…—well, caveat everyone.</p>
</blockquote>
<p><a href="https://twitter.com/narrowlytaylord/status/1662097184077012994">@narrowlytaylord</a>, 26th May 2023:</p>
<blockquote>
<p>two attorneys at my firm had opposing counsel file ChatGPT briefs with fake cases this past week</p>
<p>[...]</p>
<p>(1) They aren’t my matters so I don’t know how comfortable I am sharing much more detail</p>
<p>(2) One was an opposition to an MTD, and the state, small claims court judge did not care at the “your honor these cases don’t exist” argument <g-emoji class="g-emoji" alias="face_with_spiral_eyes" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f635-1f4ab.png">😵‍💫</g-emoji></p>
</blockquote>
<div class="entryFooter">
<a href="/2023/May/27/lawyer-chatgpt/" title="Permalink for &quot;Lawyer cites fake cases invented by ChatGPT, judge is not amused&quot;">7:09 pm</a> / <a href="/2023/May/27/">27th May 2023</a> / <a href="/tags/ethics/">ethics</a>, <a href="/tags/generativeai/">generativeai</a>, <a href="/tags/openai/">openai</a>, <a href="/tags/chatgpt/">chatgpt</a>, <a href="/tags/ai/">ai</a>, <a href="/tags/llms/">llms</a>
</div>
</div> <!-- end div.entry -->


</div> <!-- #primary -->

<div id="secondary">

<h2 class="overband">Elsewhere <a href="/atom/links/" class="small-atom" title="Atom feed of new links"><svg xmlns="http://www.w3.org/2000/svg"  width="14px" height="14px" viewBox="0 0 256 256"><circle cx="68" cy="189" r="24" fill="#FFF"/><path fill="#FFF" d="M160 213h-34a82 82 0 0 0-82-82V97a116 116 0 0 1 116 116z"/><path fill="#FFF" d="M184 213A140 140 0 0 0 44 73V38a175 175 0 0 1 175 175z"/></svg></a></h2>

<div class="day">
<h3><a href="/2023/Jun/23/" title="23rd June 2023">23rd June 2023</a></h3>


<ul>
  <li data-date="2023-06-23T23:59:54+00:00">
      <blockquote><p>Every year, some generation of engineers have to learn the concepts of “there is no silver bullet”, “use the right tech for the right problem”, “your are not google”, “rewriting a codebase every 2 years is not a good business decision”, “things cost money”.</p></blockquote><p class="cite">&mdash; <a href="https://news.ycombinator.com/item?id=36429671">sametmax</a> <a href="/2023/Jun/23/sametmax/" rel="bookmark">#</a> <span class="elsewhere-date">&mdash; <a href="/2023/Jun/23/">23rd June 2023</a>, 11:59&nbsp;pm</span></p>
      
  </li>
  
</ul>

</div> <!-- .day -->

<div class="day">
<h3><a href="/2023/Jun/22/" title="22nd June 2023">22nd June 2023</a></h3>


<ul>
  <li data-date="2023-06-22T11:13:46+00:00">
      <blockquote><p>Back then [in 2012], no one was thinking about AI. You just keep uploading your images [to Adobe Stock] and you get your residuals every month and life goes on — then all of a sudden, you find out that they trained their AI on your images and on everybody’s images that they don’t own. And they’re calling it ‘ethical’ AI.</p></blockquote><p class="cite">&mdash; <a href="https://venturebeat.com/ai/adobe-stock-creators-arent-happy-with-firefly-the-companys-commercially-safe-gen-ai-tool/">Eric Urquhart</a> <a href="/2023/Jun/22/eric-urquhart/" rel="bookmark">#</a> <span class="elsewhere-date">&mdash; <a href="/2023/Jun/22/">22nd June 2023</a>, 11:13&nbsp;am</span></p>
      
  </li>
  
</ul>

</div> <!-- .day -->

<div class="day">
<h3><a href="/2023/Jun/19/" title="19th June 2023">19th June 2023</a></h3>


<ul>
  <li data-date="2023-06-19T08:30:32+00:00">
      <a href="https://danlamanna.com/posts/building-search-dsls-with-django/">Building Search DSLs with Django</a> (<a href="https://lobste.rs/s/itjx6c/building_search_dsls_with_django" title="Lobsters">via</a>) Neat tutorial by Dan Lamanna: how to build a GitHub-style search feature—supporting modifiers like “is:open author:danlamanna”—using PyParsing and the Django ORM. <a href="/2023/Jun/19/building-search-dsls-with-django/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/19/">19th June 2023</a>, 8:30&nbsp;am</span>
      
  </li>
  
</ul>

</div> <!-- .day -->

<div class="day">
<h3><a href="/2023/Jun/17/" title="17th June 2023">17th June 2023</a></h3>


<ul>
  <li data-date="2023-06-17T22:58:34+00:00">
      <a href="https://llm.datasette.io/en/stable/changelog.html#v0-4">LLM 0.4</a>. I released a major update to my LLM CLI tool today—version 0.4, which adds conversation mode and prompt templates so you can store and re-use interesting prompts, plus a whole bunch of other large and small improvements.<br><br>I also released 0.4.1 with some minor fixes and the ability to install the tool using Hombrew: brew install simonw/llm/llm <a href="/2023/Jun/17/llm/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/17/">17th June 2023</a>, 10:58&nbsp;pm</span>
      
  </li>
  
  <li data-date="2023-06-17T22:42:13+00:00">
      <a href="https://antonz.org/sqlean-py/">sqlean.py: Python&#x27;s sqlite3 with extensions</a>. Anton Zhiyanov built a new Python package which bundles a fresh, compiled copy of SQLite with his SQLean family of C extensions built right in. Installing it gets you the latest SQLite—3.42.0—with nearly 200 additional functions, including things like define() and eval(), fileio_read() and fileio_write(), percentile_95() and uuid4() and many more. “import sqlean as sqlite3” works as a drop-in replacement for the module from the standard library. <a href="/2023/Jun/17/sqleanpy/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/17/">17th June 2023</a>, 10:42&nbsp;pm</span>
      
  </li>
  
</ul>

</div> <!-- .day -->

<div class="day">
<h3><a href="/2023/Jun/15/" title="15th June 2023">15th June 2023</a></h3>


<ul>
  <li data-date="2023-06-15T20:16:42+00:00">
      <a href="https://www.youtube.com/watch?v=omobajJmyIU">When Zeppelins Ruled The Earth</a> (<a href="https://til.simonwillison.net/macos/imovie-slides-and-audio" title="TIL: Syncing slide images and audio in iMovie">via</a>) 15 years ago I put together a talk about the history of Zeppelins which I presented a bunch of different times in various different configurations. As far as I know there are no existing videos of it, but I found an MP3 recording today and decided to splice it together with the slides to create a video of the 6m47s version I gave at the Skillswap on Speed lightning talks event in Brighton on the 28th October 2008.<br><br>Notes on how I edited the video together using iMovie in the via link. <a href="/2023/Jun/15/when-zeppelins-ruled-the-earth/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/15/">15th June 2023</a>, 8:16&nbsp;pm</span>
      
  </li>
  
</ul>

</div> <!-- .day -->

<div class="day">
<h3><a href="/2023/Jun/14/" title="14th June 2023">14th June 2023</a></h3>


<ul>
  <li data-date="2023-06-14T20:57:08+00:00">
      <a href="https://gist.github.com/kylemcdonald/dbac21de2d7855633689f5526225154c">Example of OpenAI function calling API to extract data from LAPD newsroom articles</a> (<a href="https://twitter.com/kcimc/status/1668789461780668416" title="@kcimc">via</a>) Fascinating code example from Kyle McDonald. The OpenAI functions mechanism is intended to drive custom function calls, but I hadn’t quite appreciated how useful it can be ignoring the function calls entirely. Kyle instead uses it to define a schema for data he wants to extract from a news article, then uses the gpt-3.5-turbo-0613 to get back that exact set of extracted data as JSON. <a href="/2023/Jun/14/extracting-data/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/14/">14th June 2023</a>, 8:57&nbsp;pm</span>
      
  </li>
  
  <li data-date="2023-06-14T19:23:38+00:00">
      <a href="https://www.latent.space/p/function-agents">Emergency Pod: OpenAI&#x27;s new Functions API, 75% Price Drop, 4x Context Length</a> (<a href="https://twitter.com/swyx/status/1669043021806198784" title="@swyx">via</a>) I participated in a Twitter Spaces conversation last night about the new OpenAI functions mechanism. The recording has now been turned into a Latent Space podcast, and swyx has accompanied the recording with a detailed write-up of the different topics we covered. <a href="/2023/Jun/14/emergency-pod/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/14/">14th June 2023</a>, 7:23&nbsp;pm</span>
      
  </li>
  
</ul>

</div> <!-- .day -->

<div class="day">
<h3><a href="/2023/Jun/13/" title="13th June 2023">13th June 2023</a></h3>


<ul>
  <li data-date="2023-06-13T22:37:29+00:00">
      <a href="https://observablehq.com/@simonw/llama-encoder-and-decoder">Llama encoder and decoder</a>. I forked my GPT tokenizer Observable notebook to create a similar tool for exploring the tokenization scheme used by the Llama family of LLMs, using the new llama-tokenizer-js JavaScript library. <a href="/2023/Jun/13/llama-encoder-and-decoder/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/13/">13th June 2023</a>, 10:37&nbsp;pm</span>
      
  </li>
  
  <li data-date="2023-06-13T17:34:29+00:00">
      <a href="https://openai.com/blog/function-calling-and-other-api-updates">OpenAI: Function calling and other API updates</a>. Huge set of announcements from OpenAI today. A bunch of price reductions, but the things that most excite me are the new gpt-3.5-turbo-16k model which offers a 16,000 token context limit (4x the existing 3.5 turbo model) at a price of $0.003 per 1K input tokens and $0.004 per 1K output tokens—1/10th the price of GPT-4 8k.<br><br>The other big new feature: functions! You can now send JSON schema defining one or more functions to GPT 3.5 and GPT-4—those models will then return a blob of JSON describing a function they want you to call (if they determine that one should be called). Your code executes the function and passes the results back to the model to continue the execution flow.<br><br>This is effectively an implementation of the ReAct pattern, with models that have been fine-tuned to execute it.<br><br>They acknowledge the risk of prompt injection (though not by name) in the post: “We are working to mitigate these and other risks. Developers can protect their applications by only consuming information from trusted tools and by including user confirmation steps before performing actions with real-world impact, such as sending an email, posting online, or making a purchase.” <a href="/2023/Jun/13/function-calling/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/13/">13th June 2023</a>, 5:34&nbsp;pm</span>
      
  </li>
  
</ul>

</div> <!-- .day -->

<div class="day">
<h3><a href="/2023/Jun/12/" title="12th June 2023">12th June 2023</a></h3>


<ul>
  <li data-date="2023-06-12T18:13:08+00:00">
      <blockquote><p>Cellphones are the worst thing that’s ever happened to movies. It’s awful. [...] I think you could talk to a hundred storytellers and they would all tell you the same thing. It’s so hard to manufacture drama when everybody can get a hold of everybody all the time. It’s just not as fun as in the old days when the phone would ring and you didn’t know who was calling.</p></blockquote><p class="cite">&mdash; <a href="https://variety.com/2023/film/news/steven-soderbergh-full-circle-miniseries-upcoming-projects-ai-writers-strike-1235640731/">Steven Soderbergh</a> <a href="/2023/Jun/12/steven-soderbergh/" rel="bookmark">#</a> <span class="elsewhere-date">&mdash; <a href="/2023/Jun/12/">12th June 2023</a>, 6:13&nbsp;pm</span></p>
      
  </li>
  
</ul>

</div> <!-- .day -->

<div class="day">
<h3><a href="/2023/Jun/8/" title="8th June 2023">8th June 2023</a></h3>


<ul>
  <li data-date="2023-06-08T21:06:05+00:00">
      <a href="https://github.com/minimaxir/simpleaichat">simpleaichat</a> (<a href="https://twitter.com/minimaxir/status/1666828520981692416" title="@minimaxir">via</a>) Max Woolf released his own Python package for building against the GPT-3.5 and GPT-4 APIs (and potentially other LLMs in the future).<br><br>It’s a very clean piece of API design with some useful additional features: there’s an AsyncAIChat subclass that works with Python asyncio, and the library includes a mechanism for registering custom functions that can then be called by the LLM as tools.<br><br>One trick I haven’t seen before: it uses a combination of max_tokens: 1 and a ChatGPT logit_bias to ensure that answers to one of its default prompts are restricted to just numerals between 0 and 9. This is described in the PROMPTS.md file. <a href="/2023/Jun/8/simpleaichat/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/8/">8th June 2023</a>, 9:06&nbsp;pm</span>
      
  </li>
  
  <li data-date="2023-06-08T09:29:42+00:00">
      <a href="https://twitter.com/goodside/status/1666598580319035392">Examples of weird GPT-4 behavior for the string &quot; davidjl&quot;</a>. GPT-4, when told to repeat or otherwise process the string “ davidjl” (note the leading space character), treats it as “jndl” or “jspb” or “JDL” instead. It turns out “ davidjl” has its own single token in the tokenizer: token ID 23282, presumably dating back to the GPT-2 days.<br><br>Riley Goodside refers to these as “glitch tokens”.<br><br>This token might refer to Reddit user davidjl123 who ranks top of the league for the old /r/counting subreddit, with 163,477 posts there which presumably ended up in older training data. <a href="/2023/Jun/8/davidjl/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/8/">8th June 2023</a>, 9:29&nbsp;am</span>
      
  </li>
  
  <li data-date="2023-06-08T06:16:53+00:00">
      <a href="https://daringfireball.net/2023/06/first_impressions_of_vision_pro_and_visionos">First Impressions of Vision Pro and VisionOS</a>. John Gruber’s description of his thirty minute Vision Pro demo includes a bunch of details I haven’t seen described anywhere else, including how calibration and corrective lenses work and how precise and stable the overlays of additional information are. <a href="/2023/Jun/8/first-impressions-of-vision-pro-and-visionos/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/8/">8th June 2023</a>, 6:16&nbsp;am</span>
      
  </li>
  
  <li data-date="2023-06-08T04:59:05+00:00">
      <a href="https://matt-rickard.com/chatgpt-plugins-dont-have-pmf">ChatGPT Plugins Don&#x27;t Have PMF</a>. Sam Altman was recently quoted (in a since unpublished blog post) noting that ChatGPT plugins have not yet demonstrated product market fit.<br><br>This matches my own usage patterns: I use the “browse” and “code interpreter” modes on a daily basis, but I’ve not found any of the third party developer plugins to stick for me yet.<br><br>I like Matt Rickard’s observation here: “Chat is not the right UX for plugins. If you know what you want to do, it’s often easier to just do a few clicks on the website. If you don’t, just a chat interface makes it hard to steer the model toward your goal.” <a href="/2023/Jun/8/chatgpt-plugins-dont-have-pmf/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/8/">8th June 2023</a>, 4:59&nbsp;am</span>
      
  </li>
  
</ul>

</div> <!-- .day -->

<div class="day">
<h3><a href="/2023/Jun/5/" title="5th June 2023">5th June 2023</a></h3>


<ul>
  <li data-date="2023-06-05T16:58:17+00:00">
      <blockquote><p>If you give feedback that isn’t constructive your feedback is worthless. I know that sounds harsh but it is. If you give unconstructive feedback you might as well not be saying anything. If you just look at something and go “That’s stupid” or “I don’t like that”—that’s worthless feedback, nobody can do anything with that. They’re not going to start throwing darts against the wall until you say “Oh OK, I like that”. You have to say something more.</p></blockquote><p class="cite">&mdash; <a href="https://youtu.be/ohHLUKj3NTk?t=941">Timothy Cain</a> <a href="/2023/Jun/5/timothy-cain/" rel="bookmark">#</a> <span class="elsewhere-date">&mdash; <a href="/2023/Jun/5/">5th June 2023</a>, 4:58&nbsp;pm</span></p>
      
  </li>
  
  <li data-date="2023-06-05T15:49:27+00:00">
      <a href="https://twitter.com/OfficialLoganK/status/1663934947931897857">Logan Kilpatrick (OpenAI)</a>. “The API does not just change without us telling you. The models are static there.”<br><br>That’s the official line on the ongoing questions concerning whether OpenAI’s models have been degrading in quality over the last few weeks and months.<br><br>Worth noting that this mentions the API but doesn’t mention ChatGPT itself, which I suspect gets model updates a lot more frequently than the models served through the API. <a href="/2023/Jun/5/logan-kilpatrick-openai/" rel="bookmark">#</a><span class="elsewhere-date"> &mdash; <a href="/2023/Jun/5/">5th June 2023</a>, 3:49&nbsp;pm</span>
      
  </li>
  
</ul>

</div> <!-- .day -->

<script>
/* Rearrange content on smaller screens */
window.hasBeenRearrangedForMobile = false;
function rearrangeForMobile() {
  document.querySelector('#secondary').classList.add('hide-secondary-on-mobile');
  const items = Array.from(document.querySelectorAll('*[data-date]'));
  items.sort((a, b) => {
    const aDate = a.getAttribute('data-date');
    const bDate = b.getAttribute('data-date');
    if (aDate < bDate) {
      return 1;
    }
    if (aDate > bDate) {
      return -1;
    }
    return 0;
  });
  const primary = document.querySelector('#primary');
  items.forEach(el => {
    if (el.tagName == 'LI') {
      // Turn this into a <div>
      var div = document.createElement('div');
      div.classList.add('segment');
      div.classList.add('elsewhere-in-primary');
      if (el.querySelector('blockquote')) {
        div.classList.add('quote');
      }
      div.innerHTML = el.innerHTML;
      primary.appendChild(div);
    } else {
      primary.appendChild(el);
    }
  });
}
function conditionalRearrange(m) {
  if (m.matches && !window.hasBeenRearrangedForMobile) {
    rearrangeForMobile();
    window.hasBeenRearrangedForMobile = true;
  }
}
var mediaMatcher = window.matchMedia('(max-width: 800px)');
conditionalRearrange(mediaMatcher);
mediaMatcher.addListener(conditionalRearrange);
</script>

</div> <!-- #secondary -->
</div> <!-- #wrapper -->



<div id="ft">
    <ul>
      <li><a href="https://github.com/simonw/simonwillisonblog">Source code</a></li>
      <li>&copy;</li>
      <li><a href="/2002/">2002</a></li>
      <li><a href="/2003/">2003</a></li>
      <li><a href="/2004/">2004</a></li>
      <li><a href="/2005/">2005</a></li>
      <li><a href="/2006/">2006</a></li>
      <li><a href="/2007/">2007</a></li>
      <li><a href="/2008/">2008</a></li>
      <li><a href="/2009/">2009</a></li>
      <li><a href="/2010/">2010</a></li>
      <li><a href="/2011/">2011</a></li>
      <li><a href="/2012/">2012</a></li>
      <li><a href="/2013/">2013</a></li>
      <li><a href="/2014/">2014</a></li>
      <li><a href="/2015/">2015</a></li>
      <li><a href="/2016/">2016</a></li>
      <li><a href="/2017/">2017</a></li>
      <li><a href="/2018/">2018</a></li>
      <li><a href="/2019/">2019</a></li>
      <li><a href="/2020/">2020</a></li>
      <li><a href="/2021/">2021</a></li>
      <li><a href="/2022/">2022</a></li>
      <li><a href="/2023/">2023</a></li>
    </ul>
</div>

</body>
</html>
